{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What you've seen before\n",
    "import requests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# What's new\n",
    "from bs4 import BeautifulSoup # We'll be using this to scrape through HTML documents\n",
    "import re # We'll be using regular expressions (a bit) to search through text\n",
    "import networkx as nx # We'll be using this to graph a website (not super important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Hacker News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_doc = requests.get('https://news.ycombinator.com/').text\n",
    "html_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Document Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc)\n",
    "print(soup.title) # Find the 'title' tag\n",
    "print(soup.title.string) # Find the 'title' tag's text value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can get all the text from elements in the HTML\n",
    "# to get an easy dataset for the webpage's text content\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.a # This gives us the first link ('a') tag in the document\n",
    "\n",
    "# We can also iterate over all the link tags in a document\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppose we know the class of an element we want to search for.\n",
    "# We can pass that in as a parameter to narrow our search\n",
    "for story_link in soup.find_all('a', {'class': 'storylink'}):\n",
    "    print(story_link.text, story_link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories = []\n",
    "for news_row in soup.find_all('tr', {'class': 'athing'}):\n",
    "    # We can use the same commands on children nodes of the document\n",
    "    # (.find() returns the first found result)\n",
    "    story = news_row.find('a', {'class': 'storylink'})\n",
    "    story_name, story_link = story.text, story.get('href')\n",
    "\n",
    "    # We're not restricted just to searching up and down the document,\n",
    "    # We can search accross 'siblings'\n",
    "    meta_data_row = news_row.find_next_sibling('tr')\n",
    "    \n",
    "    score, comments = None, None\n",
    "    \n",
    "    # Need to check if the story has a score\n",
    "    if meta_data_row.find('span', {'class': 'score'}):\n",
    "        score = int(meta_data_row.find('span', {'class': 'score'}).text.split()[0])\n",
    "    \n",
    "    # If you know regular expressions, you can use those to search for patterns\n",
    "    # in the document\n",
    "    comments_pattern = re.compile(r'comments')\n",
    "    if meta_data_row.find('a', text=comments_pattern):\n",
    "        comments = int(meta_data_row.find('a', text=comments_pattern).text.split()[0])\n",
    "    \n",
    "    stories.append({\n",
    "            'name': story_name,\n",
    "            'link': story_link,\n",
    "            'score': score,\n",
    "            'comments': comments\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories_df = pd.DataFrame(stories)\n",
    "stories_df.plot.scatter('comments', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Crawling the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyt_url = 'http://www.nytimes.com/'\n",
    "nyt_links = {} # Dict to hold adjacent sites\n",
    "links_to_scrape = [nyt_url] # Stack of links to visit\n",
    "\n",
    "# Keep going while we still have links to visit, and we have\n",
    "# 'seen' less than 1500 (unique) links\n",
    "while links_to_scrape and len(nyt_links) < 1000:\n",
    "    # Grab the 'latest' link from our list\n",
    "    scrape_page = links_to_scrape.pop()\n",
    "    # Visit our link and make it into soup\n",
    "    soup = BeautifulSoup(requests.get(scrape_page).text)\n",
    "    \n",
    "    # Iterate through all the links on the current page\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href') # Get the link's address\n",
    "        \n",
    "        # Reject the link if invalid or doesn't start with 'http'\n",
    "        if not href or not href.startswith('http'):\n",
    "            continue\n",
    "            \n",
    "        # Have we already visited this link?\n",
    "        if not href in nyt_links:\n",
    "            nyt_links[href] = [scrape_page] # Initialize list of adjacent links\n",
    "            \n",
    "            # Save link to scrape later\n",
    "            if href not in links_to_scrape:\n",
    "                links_to_scrape.append(href)\n",
    "        else:\n",
    "            nyt_links[href].append(scrape_page) # Add link to adjacent links\n",
    "            \n",
    "    # Report how many links we've 'seen'\n",
    "    print(\"Up to {} links.\".format(len(nyt_links)))\n",
    "\n",
    "print nyt_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the graph (not super important to know in detail)\n",
    "\n",
    "# Create a graph and feed it in our pages as nodes\n",
    "nyt_graph = nx.Graph()\n",
    "nyt_graph.add_nodes_from(nyt_links.keys())\n",
    "\n",
    "# Add edges based on the 'adjacent' lists we created earlier\n",
    "for node, adjacent in nyt_links.items():\n",
    "    for a in adjacent:\n",
    "        nyt_graph.add_edge(node, a)\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    fig = plt.subplots(1, figsize=(12,10))\n",
    "    nx.draw_networkx(nyt_graph, edge_color='#a4a4a4', with_labels=False,\n",
    "                     node_size=map(lambda x: len(x) * 10, nyt_links.values()))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "#### These may take you a bit longer! ðŸ˜‰\n",
    "\n",
    "## Challenge 1: Subreddit Scraper\n",
    "Write a function that, given the name of a subreddit, will return you a DataFrame with the same data that we scraped from the HackerNews news page. (Story name / link, Score, and # of comments).\n",
    "\n",
    "(i.e. `subreddit_scraper('dataisbeautiful')` would be the function call)\n",
    "\n",
    "**NOTE**: You may want to use this in your request: `headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subreddit_scraper(sub_str):\n",
    "    pass\n",
    "\n",
    "subreddit_scraper('dataisbeautiful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2: PyPI Parser\n",
    "*(PyPI is the indexer of popular python packages)*\n",
    "\n",
    "Write a function that, given the URL of a Python package, will return you a dictionary with the following package information:\n",
    "- Author\n",
    "- Release (upload) Date\n",
    "- License\n",
    "- Home Page\n",
    "\n",
    "(Note the possibility that not all of these fields will be present on all packages, and write your function accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pypi_parser(pypi_url):\n",
    "    pass\n",
    "\n",
    "pypi_parser('https://pypi.python.org/pypi/beautifulsoup4/4.5.1')\n",
    "pypi_parser('https://pypi.python.org/pypi/SQLAlchemy/1.1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: IMDB Cast Data\n",
    "Write a function that, given the URL of a movie on IMDB will return you a list of the cast of the movie with the following information:\n",
    "- Actor name\n",
    "- Character name\n",
    "- Actor birthday\n",
    "\n",
    "Example:\n",
    "```\n",
    ">>> imdb_cast('http://www.imdb.com/title/tt0796366/')\n",
    "[\n",
    "  {'actor': 'Chris Pine',\n",
    "  'character': 'Kirk',\n",
    "  'birthday': 'August 26, 1980'\n",
    "  }\n",
    "  ...\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imdb_cast(movie_url):\n",
    "    pass\n",
    "imdb_cast('http://www.imdb.com/title/tt0796366/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2><center>That's it!</center></h2>\n",
    "![nice_job](http://i.giphy.com/eoxomXXVL2S0E.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Points:\n",
    "1. For problem 1, write another function that uses the output of your function to plot the # of comments against the score of the post. Also, include a linear trend line. (Hint: `numpy.polyfit`)\n",
    "2. For problem 2, include an array of all the published versions of the package in your return dictionary.\n",
    "\n",
    "*Important:* When you send us your solutions, let us know which (if any)\n",
    "of the bonus problems you've attempted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
