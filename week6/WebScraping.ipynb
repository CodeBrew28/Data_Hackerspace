{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Hacker News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html_doc = requests.get('https://news.ycombinator.com/').text\n",
    "html_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Document Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc)\n",
    "print(soup.title) # Find the 'title' tag\n",
    "print(soup.title.string) # Find the 'title' tag's text value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can get all the text from elements in the HTML\n",
    "# to get an easy dataset for the webpage's text content\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print soup.a # This gives us the first link ('a') tag in the document\n",
    "\n",
    "# We can also iterate over all the link tags in a document\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Suppose we know the class of an element we want to search for.\n",
    "# We can pass that in as a parameter to narrow our search\n",
    "for story_link in soup.find_all('a', {'class': 'storylink'}):\n",
    "    print(story_link.text, story_link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories = []\n",
    "for news_row in soup.find_all('tr', {'class': 'athing'}):\n",
    "    # We can use the same commands on children nodes of the document\n",
    "    # (.find() returns the first found result)\n",
    "    story = news_row.find('a', {'class': 'storylink'})\n",
    "    story_name, story_link = story.text, story.get('href')\n",
    "\n",
    "    # We're not restricted just to searching up and down the document,\n",
    "    # We can search accross 'siblings'\n",
    "    meta_data_row = news_row.find_next_sibling('tr')\n",
    "    \n",
    "    score, comments = None, None\n",
    "    \n",
    "    # Need to check if the story has a score\n",
    "    if meta_data_row.find('span', {'class': 'score'}):\n",
    "        score = int(meta_data_row.find('span', {'class': 'score'}).text.split()[0])\n",
    "    \n",
    "    # If you know regular expressions, you can use those to search for patterns\n",
    "    # in the document\n",
    "    comments_pattern = re.compile(r'comments')\n",
    "    if meta_data_row.find('a', text=comments_pattern):\n",
    "        comments = int(meta_data_row.find('a', text=comments_pattern).text.split()[0])\n",
    "    \n",
    "    stories.append({\n",
    "            'name': story_name,\n",
    "            'link': story_link,\n",
    "            'score': score,\n",
    "            'comments': comments\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories_df = pd.DataFrame(stories)\n",
    "stories_df.plot.scatter('comments', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Crawling the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "#### These may take you a bit longer! ðŸ˜‰\n",
    "\n",
    "## Challenge 1:\n",
    "Write a function that, given the name of a subreddit, will return you a DataFrame with the same data that we scraped from the HackerNews news page. (Story name / link, Score, and # of comments).\n",
    "\n",
    "(i.e. `subreddit_scraper('dataisbeautiful')` would be the function call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def subreddit_scraper(sub_str):\n",
    "    pass\n",
    "\n",
    "subreddit_scraper('dataisbeautiful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2><center>That's it!</center></h2>\n",
    "![nice_job](http://i.giphy.com/eoxomXXVL2S0E.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
